---
layout: post
title: "PyTorch"
date: 2025-09-21
categories: ML
---
![PyTorch logo](/assets/images/posts/2025/pytorch/pytorch_logo.png)
## ðŸ“Œ Introduction
In this blog post, I will walk you through the basics of Pytorch, an open-source deep learning frame work which is used to build AI products such as [Tesla Autopilot](https://www.youtube.com/watch?v=oBklltKXtDE), [GPT-3](https://openai.com/index/gpt-3-apps/), and [Stable Diffusion](https://stability.ai/stable-image). It's very popular because:
- It's easy to understand like Python
- Has a strong open-source community
- Mainly used in research

Before we start, this is how to import PyTorch:
```python
import torch
```

## ðŸ”¢ Tensors
I think that it's pretty reasonable to think pytorch as:<br>
***PyTorch = Python + Tensors + Autograd***<br>
The first thing I will talk about are tensors.<br>
Tensors are the heart of PyTorch; but it's actually just an arrary that has several useful features.<br>
This is how you initialize a tensor:
```python
x = torch.tensor([3.14, 1.59])
print(x)
```
Output:
```text
tensor([3.1400, 1.5900])
```
Now, I will introduce you some basic functions of tensors:
```python
# uninitialized
x = torch.empty(2, 3)
print(f"empty(2, 3):\n{x}\n")

# random
x = torch.rand(4, 2)
print(f"rand(4, 2):\n{x}\n")

# zeros, ones
x = torch.zeros(3)
print(f"zeros(3):\n{x}\n")
x = torch.ones(2, 3)
print(f"ones(2, 3):\n{x}\n")
```
Output:
```text
empty(2, 3):
tensor([[0., 0., 0.],
        [0., 0., 0.]])

rand(4, 2):
tensor([[0.6997, 0.2806],
        [0.2932, 0.3877],
        [0.0806, 0.2674],
        [0.5697, 0.7124]])

zeros(3):
tensor([0., 0., 0.])

ones(2, 3):
tensor([[1., 1., 1.],
        [1., 1., 1.]])
```

Here you might ask, "What's the difference between `torch.empty` and `torch.zeros`?" The difference is that `torch.empty` creates tensors without initializes the values and `torch.zeros` creates tensors by filling it with zeros. Because of the intialization step, `torch.empty` runs quicker than `torch.zeros`. The difference might not even be noticable, but in large scale, this can save lot of time.

This are some functions that allows us to modify and know sizes and data types of tensors:
```python
# size
print(f"size:\n{x.size()}\n")
print(f"shape:\n{x.shape}\n")

# data type
print(f"data type:\n{x.dtype}\n")

# specify types (float32 is default)
x = torch.ones(1, 2, dtype=torch.float16)
print(f"new data type:\n{x.dtype}\n")
```
```text
zeros(3):
tensor([0., 0., 0.])

ones(2, 3):
tensor([[1., 1., 1.],
        [1., 1., 1.]])

size:
torch.Size([2, 3])

shape:
torch.Size([2, 3])

data type:
torch.float32

new data type:
torch.float16
```

In machine learning, linear algebra is used a lot, since it allows compuations to run more effiecently. As a machine learning library, PyTorch allows us to perform linear operations with tensors, and here are some examples (which are pretty straightforward):
```python
x = torch.ones(2, 2)
y = torch.rand(2, 2)

# add
z = x + y
print(z)

# subtract
z = x - y
print(z)

# multiply
z = x * y
print(z)

# divide
z = x / y
print(z)
```
Output:
```text
tensor([[1.6110, 1.0638],
        [1.9629, 1.8989]])
tensor([[0.3890, 0.9362],
        [0.0371, 0.1011]])
tensor([[0.6110, 0.0638],
        [0.9629, 0.8989]])
tensor([[ 1.6368, 15.6769],
        [ 1.0386,  1.1125]])
```

These are some other functions that might be useful:
```python
x = torch.rand(5,3)
print(x)

# Slicing
print("x[:, 0]", x[:, 0]) # all rows, column 0
print("x[1, :]", x[1, :]) # row 1, all columns
print("x[1, 1]", x[1, 1]) # element at 1, 1

# Reshape
y = x.view(2, 8)
print(y)

z = x.view(16, -1)
print(z)

# torch to numpy
y = x.numpy()
print(b)
print(type(b))